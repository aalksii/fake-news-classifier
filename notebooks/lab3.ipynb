{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory work #3 (text vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aleksei/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aleksei/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import string\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.errors import EmptyDataError\n",
    "import numpy as np\n",
    "from math import log1p\n",
    "import gensim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(root_dir, n=None):\n",
    "    file_paths = []\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.tsv'):\n",
    "                file_paths.append(os.path.join(subdir, file))\n",
    "\n",
    "    data = []\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        if n is not None and i >= n:\n",
    "            break\n",
    "        try:\n",
    "            d = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "            d.columns = ['Token', 'Stem', 'Lemma']\n",
    "        except EmptyDataError as e:\n",
    "            print(i, file_path, e)\n",
    "        data.append(d.dropna())\n",
    "        \n",
    "    \n",
    "    ids = [os.path.splitext(os.path.basename(path))[0] for path in file_paths]\n",
    "    return ids, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, train = read_files('../assets/annotated-corpus/train', \n",
    "                   1000\n",
    "                   )\n",
    "# val = read_files('../assets/annotated-corpus/val', \n",
    "#                 #  100\n",
    "#                  )\n",
    "test_ids, test = read_files('../assets/annotated-corpus/test', \n",
    "                  100\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Stem</th>\n",
       "      <th>Lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WASHINGTON</td>\n",
       "      <td>washington</td>\n",
       "      <td>WASHINGTON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reuters</td>\n",
       "      <td>reuter</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The</td>\n",
       "      <td>the</td>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U</td>\n",
       "      <td>u</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Token        Stem       Lemma\n",
       "0  WASHINGTON  washington  WASHINGTON\n",
       "1     Reuters      reuter     Reuters\n",
       "2           -           -           -\n",
       "3         The         the         The\n",
       "4           U           u           U"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_token(token, token_frequencies, min_frequency=2):\n",
    "    if token in string.punctuation:\n",
    "        return False\n",
    "    if token.lower() in stop_words:\n",
    "        return False\n",
    "    if token_frequencies[token] < min_frequency:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_freqs(dfs):\n",
    "    token_frequencies = Counter()\n",
    "    term_document_matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for doc_id, df in enumerate(dfs):\n",
    "        tokens = df['Token'].tolist()\n",
    "        token_frequencies.update(tokens)\n",
    "\n",
    "        for token in tokens:\n",
    "            if is_valid_token(token, token_frequencies):\n",
    "                term_document_matrix[doc_id][token] += 1\n",
    "                \n",
    "    # filter all tokens that return is_valid_token False\n",
    "    token_frequencies = Counter(dict({(token, freq) for (token, freq) in token_frequencies.items() if is_valid_token(token, token_frequencies)}))\n",
    "\n",
    "    for doc_id, terms in term_document_matrix.items():\n",
    "        term_document_matrix[doc_id] = {token: freq for token, freq in terms.items() if is_valid_token(token, token_frequencies)}\n",
    "\n",
    "    return token_frequencies, term_document_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 4498),\n",
       " ('Trump', 2440),\n",
       " ('U', 1938),\n",
       " ('would', 1439),\n",
       " ('Reuters', 1368),\n",
       " ('President', 882),\n",
       " ('government', 843),\n",
       " ('Republican', 751),\n",
       " ('also', 705),\n",
       " ('House', 697),\n",
       " ('United', 685),\n",
       " ('people', 658),\n",
       " ('told', 646),\n",
       " ('could', 644),\n",
       " ('state', 598),\n",
       " ('States', 587),\n",
       " ('percent', 543),\n",
       " ('year', 534),\n",
       " ('two', 520),\n",
       " ('last', 516)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_frequencies, term_document_matrix = get_freqs(train)\n",
    "token_frequencies.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Reuters': 3,\n",
       " 'U': 3,\n",
       " 'State': 3,\n",
       " 'Department': 2,\n",
       " 'certified': 2,\n",
       " 'government': 3,\n",
       " 'corruption': 3,\n",
       " 'rights': 2,\n",
       " 'Honduras': 8,\n",
       " 'receive': 2,\n",
       " 'millions': 2,\n",
       " 'dollars': 2,\n",
       " 'aid': 2,\n",
       " 'document': 2,\n",
       " 'seen': 3,\n",
       " 'showed': 2,\n",
       " 'Monday': 2,\n",
       " 'election': 3,\n",
       " 'violent': 2,\n",
       " 'winner': 2,\n",
       " 'week': 2,\n",
       " 'certification': 2,\n",
       " 'congressional': 2,\n",
       " 'President': 4,\n",
       " 'Trump': 2,\n",
       " 'administration': 2,\n",
       " 'taking': 3,\n",
       " 'one': 3,\n",
       " 'officials': 2,\n",
       " 'requirements': 2,\n",
       " 'Congress': 2,\n",
       " 'governments': 2,\n",
       " 'including': 2,\n",
       " 'former': 4,\n",
       " 'opposition': 2,\n",
       " 'Salvador': 2,\n",
       " 'victory': 2,\n",
       " 'Hernandez': 4,\n",
       " 'Nasralla': 3}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_document_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../assets/data/')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "with open(data_dir / 'token_frequencies.tsv', 'w', encoding='utf-8') as file:\n",
    "    for token, freq in token_frequencies.items():\n",
    "        if is_valid_token(token, token_frequencies):\n",
    "            file.write(f'{token}\\t{freq}\\n')\n",
    "\n",
    "with open(data_dir / 'term_document_matrix.tsv', 'w', encoding='utf-8') as file:\n",
    "    for doc_id, terms in term_document_matrix.items():\n",
    "        for token, freq in terms.items():\n",
    "            file.write(f'{doc_id}\\t{token}\\t{freq}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_document_df = pd.DataFrame.from_dict(term_document_matrix, orient='index').fillna(0)\n",
    "term_document_df = term_document_df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Reuters</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Department</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>certified</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0    1    2    3    4    5    6    7    8    9    ...  990  991  \\\n",
       "Reuters     3.0  1.0  1.0  1.0  1.0  1.0  2.0  1.0  2.0  1.0  ...  2.0  1.0   \n",
       "U           3.0  0.0  3.0  0.0  0.0  6.0  0.0  0.0  2.0  1.0  ...  1.0  3.0   \n",
       "State       3.0  3.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  0.0  ...  0.0  1.0   \n",
       "Department  2.0  0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "certified   2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "            992  993  994  995  996  997  998  999  \n",
       "Reuters     1.0  1.0  1.0  1.0  1.0  1.0  1.0  2.0  \n",
       "U           1.0  1.0  0.0  4.0  1.0  1.0  1.0  2.0  \n",
       "State       0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "Department  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  \n",
       "certified   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_document_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    def split_into_sentences(text):\n",
    "        # so the website will not split into two separate sentences by comma:\n",
    "        sentence_endings = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!)(?=\\s|[#])')\n",
    "        sentences = sentence_endings.split(text)\n",
    "        sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "        return sentences\n",
    "    \n",
    "    def split_into_words(sentences):\n",
    "        # regular expression to match complex URLs, simple URLs, hashtags, Twitter handles, and words\n",
    "        word_pattern = re.compile(r'pic.twitter.com/\\S+|https?://\\S+|www\\.\\S+|\\#\\S+|\\@\\w+|\\b\\w+\\'?\\w*|-?\\w+\\'?\\w*')\n",
    "        tokenized_sentences = []\n",
    "        for sentence in sentences:\n",
    "            words = word_pattern.findall(sentence)\n",
    "            tokenized_sentences.append(words)\n",
    "        return tokenized_sentences\n",
    "\n",
    "    sentences = split_into_sentences(text)\n",
    "    tokenized = split_into_words(sentences)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(sentence_tokens, token):\n",
    "    return sentence_tokens.count(token) / len(sentence_tokens)\n",
    "\n",
    "\n",
    "def compute_idf(token, term_document_matrix, total_documents):\n",
    "    doc_count = sum(1 for doc in term_document_matrix if token in term_document_matrix[doc])\n",
    "    return log1p(total_documents / (1 + doc_count))\n",
    "\n",
    "\n",
    "def process_text_and_create_matrices(text, token_frequencies, term_document_matrix):\n",
    "    tokenized_sentences = preprocess_text(text)\n",
    "    total_documents = len(term_document_matrix)\n",
    "    vocabulary = sorted(token_frequencies.keys())\n",
    "\n",
    "    max_sentence_length = max(len(sentence) for sentence in tokenized_sentences)\n",
    "\n",
    "    frequency_matrix = []\n",
    "    tfidf_matrix = []\n",
    "\n",
    "    for sentence in tokenized_sentences:\n",
    "        sentence_freq_vector = [0] * max_sentence_length\n",
    "        sentence_tfidf_vector = [0] * max_sentence_length\n",
    "\n",
    "        for i, token in enumerate(sentence):\n",
    "            if token in vocabulary:\n",
    "                tf = compute_tf(sentence, token)\n",
    "                idf = compute_idf(token, term_document_matrix, total_documents)\n",
    "\n",
    "                sentence_freq_vector[i] = tf\n",
    "                sentence_tfidf_vector[i] = tf * idf\n",
    "\n",
    "        frequency_matrix.append(sentence_freq_vector)\n",
    "        tfidf_matrix.append(sentence_tfidf_vector)\n",
    "\n",
    "    frequency_matrix = np.array(frequency_matrix)\n",
    "    tfidf_matrix = np.array(tfidf_matrix)\n",
    "\n",
    "    document_vector_freq = np.mean(frequency_matrix, axis=0)\n",
    "    document_vector_tfidf = np.mean(tfidf_matrix, axis=0)\n",
    "\n",
    "    return document_vector_freq, document_vector_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boos and chants of  Lock her up!  were heard in the crowd assembled at the West Front of the U.S. Capitol Friday morning when defeated Democratic Party presidential nominee Hillary Clinton was introduced at the inaugural ceremony for President-elect Donald Trump.#InaugurationDay Lock her up pic.twitter.com/APVtyyYote  Bill Simms (@Mittens1245) January 20, 2017The crowd on the mall booed when the jumbotron showed a close-up shot of Hillary Clinton at #Inauguration https://t.co/1dvY5lxdKo  gpbnews (@gpbnews) January 20, 2017Some in crowd chanting LOCK HER UP as Hillary Clinton arrives  Jamie Dupree (@jamiedupree) January 20, 2017Via: Gateway Pundit \n"
     ]
    }
   ],
   "source": [
    "text = 'Boos and chants of  Lock her up!  were heard in the crowd assembled at the West Front of the U.S. Capitol Friday morning when defeated Democratic Party presidential nominee Hillary Clinton was introduced at the inaugural ceremony for President-elect Donald Trump.#InaugurationDay Lock her up pic.twitter.com/APVtyyYote  Bill Simms (@Mittens1245) January 20, 2017The crowd on the mall booed when the jumbotron showed a close-up shot of Hillary Clinton at #Inauguration https://t.co/1dvY5lxdKo  gpbnews (@gpbnews) January 20, 2017Some in crowd chanting LOCK HER UP as Hillary Clinton arrives  Jamie Dupree (@jamiedupree) January 20, 2017Via: Gateway Pundit '\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((53,), (53,))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vector_freq, document_vector_tfidf = process_text_and_create_matrices(text, token_frequencies, term_document_matrix)\n",
    "document_vector_freq.shape, document_vector_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00925926, 0.04761905, 0.        , 0.00925926,\n",
       "       0.00628931, 0.        , 0.        , 0.02812718, 0.02812718,\n",
       "       0.        , 0.01257862, 0.00925926, 0.        , 0.00925926,\n",
       "       0.00925926, 0.00925926, 0.        , 0.00925926, 0.01554857,\n",
       "       0.00925926, 0.01554857, 0.01554857, 0.01554857, 0.00925926,\n",
       "       0.01257862, 0.02183788, 0.        , 0.        , 0.00925926,\n",
       "       0.00925926, 0.        , 0.02812718, 0.02812718, 0.00925926,\n",
       "       0.00925926, 0.01257862, 0.00628931, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01257862, 0.01257862, 0.        ,\n",
       "       0.00628931, 0.        , 0.        , 0.01886792, 0.01886792,\n",
       "       0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vector_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.03336265, 0.29602886, 0.        , 0.03304673,\n",
       "       0.02064435, 0.        , 0.        , 0.08169652, 0.08886944,\n",
       "       0.        , 0.04489367, 0.00876609, 0.        , 0.04185926,\n",
       "       0.01586091, 0.02994782, 0.        , 0.04106275, 0.03311441,\n",
       "       0.02228737, 0.03041956, 0.04741797, 0.04646397, 0.02155723,\n",
       "       0.02993596, 0.0652484 , 0.        , 0.        , 0.04910468,\n",
       "       0.03596311, 0.        , 0.06131343, 0.07238644, 0.01099263,\n",
       "       0.01077756, 0.04489367, 0.03475128, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.02993596, 0.02928529, 0.        ,\n",
       "       0.03125044, 0.        , 0.        , 0.05196293, 0.04286184,\n",
       "       0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vector_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [[token for token in ds['Token'].to_list() if is_valid_token(token, token_frequencies)] for ds in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WASHINGTON',\n",
       " 'Reuters',\n",
       " 'U',\n",
       " 'State',\n",
       " 'Department',\n",
       " 'certified',\n",
       " 'Honduran',\n",
       " 'government',\n",
       " 'fighting',\n",
       " 'corruption']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences=train_texts, vector_size=30, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('../models/').mkdir(parents=True, exist_ok=True)\n",
    "model_path = '../models/word2vec.model'\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 351\n",
      "Close: 366 340 369\n",
      "Same area 32 136 355\n",
      "Other semantic 54 54 51\n"
     ]
    }
   ],
   "source": [
    "print('Word:', token_frequencies['Monday'])\n",
    "print('Close:', token_frequencies['Tuesday'], token_frequencies['Wednesday'], token_frequencies['Thursday'])\n",
    "print('Same area', token_frequencies['weekend'], token_frequencies['day'], token_frequencies['week'])\n",
    "print('Other semantic', token_frequencies['funds'], token_frequencies['town'], token_frequencies['territory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 30\n",
      "Close: 25 13 11\n",
      "Same area 197 59 12\n",
      "Other semantic 357 51 46\n"
     ]
    }
   ],
   "source": [
    "print('Word:', token_frequencies['north'])\n",
    "print('Close:', token_frequencies['south'], token_frequencies['west'], token_frequencies['east'])\n",
    "print('Same area', token_frequencies['world'], token_frequencies['side'], token_frequencies['direction'])\n",
    "print('Other semantic', token_frequencies['party'], token_frequencies['senator'], token_frequencies['husband'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 53\n",
      "Close: 19 49 90\n",
      "Same area 81 12 190\n",
      "Other semantic 2440 103 282\n"
     ]
    }
   ],
   "source": [
    "print('Word:', token_frequencies['Spain'])\n",
    "print('Close:', token_frequencies['Madrid'], token_frequencies['Catalonia'], token_frequencies['Europe'])\n",
    "print('Same area', token_frequencies['Brexit'], token_frequencies['kingdom'], token_frequencies['EU'])\n",
    "print('Other semantic', token_frequencies['Trump'], token_frequencies['Twitter'], token_frequencies['Korea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distances for \"Monday\":\n",
      "\tSimilar: {'Tuesday': 0.9846577, 'Wednesday': 0.9913203, 'Thursday': 0.9923759}\n",
      "\tRelated: {'weekend': 0.8830235, 'day': 0.9369939, 'week': 0.75852627}\n",
      "\tUnrelated: {'funds': 0.7935365, 'town': 0.8225489, 'territory': 0.81213}\n",
      "Cosine distances for \"north\":\n",
      "\tSimilar: {'south': 0.9954677, 'west': 0.9939249, 'east': 0.9942235}\n",
      "\tRelated: {'world': 0.975929, 'side': 0.98762476, 'direction': 0.9896573}\n",
      "\tUnrelated: {'party': 0.8913339, 'senator': 0.85640633, 'husband': 0.93527836}\n",
      "Cosine distances for \"Spain\":\n",
      "\tSimilar: {'Madrid': 0.9910841, 'Catalonia': 0.9976725, 'Europe': 0.9921143}\n",
      "\tRelated: {'Brexit': 0.99399817, 'kingdom': 0.984493, 'EU': 0.9867774}\n",
      "\tUnrelated: {'Trump': 0.60371745, 'Twitter': 0.9568, 'Korea': 0.51787007}\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec_a, vec_b):\n",
    "    dot_product = np.dot(vec_a, vec_b)\n",
    "    norm_a = np.linalg.norm(vec_a)\n",
    "    norm_b = np.linalg.norm(vec_b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "words_to_analyze = ['Monday', 'north', 'Spain']\n",
    "similar_words = {\n",
    "    'Monday': ['Tuesday', 'Wednesday', 'Thursday'], \n",
    "    'north': ['south', 'west', 'east'],\n",
    "    'Spain': ['Madrid', 'Catalonia', 'Europe']\n",
    "}\n",
    "\n",
    "related_words = {\n",
    "    'Monday': ['weekend', 'day', 'week'], \n",
    "    'north': ['world', 'side', 'direction'],\n",
    "    'Spain': ['Brexit', 'kingdom', 'EU']\n",
    "}\n",
    "\n",
    "unrelated_words = {\n",
    "    'Monday': ['funds', 'town', 'territory'], \n",
    "    'north': ['party', 'senator', 'husband'],\n",
    "    'Spain': ['Trump', 'Twitter', 'Korea']\n",
    "}\n",
    "\n",
    "for word in words_to_analyze:\n",
    "    word_vec = model.wv[word]\n",
    "    print(f'Cosine distances for \"{word}\":')\n",
    "    for group, words in [('Similar', similar_words), ('Related', related_words), ('Unrelated', unrelated_words)]:\n",
    "        distances = {target_word: cosine_similarity(word_vec, model.wv[target_word]) for target_word in words[word]}\n",
    "        print(f'\\t{group}: {distances}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distances for \"Monday\":\n",
      "\tSimilar: {'Tuesday': 0.24013497172980114, 'Wednesday': 0.09500680105219181, 'Thursday': 0.12213416677724975}\n",
      "\tRelated: {'weekend': 0.20891954827216885, 'day': 0.20346547637930487, 'week': 0.29776708853401584}\n",
      "\tUnrelated: {'funds': 0.10768513954141816, 'town': 0.1068187511177002, 'territory': 0.09110917257960921}\n",
      "Cosine distances for \"north\":\n",
      "\tSimilar: {'south': 0.24262373970773643, 'west': 0.04075695729696112, 'east': 0.14467284665112365}\n",
      "\tRelated: {'world': 0.022649412708342614, 'side': 0.04933303124557308, 'direction': 0.13794014696151088}\n",
      "\tUnrelated: {'party': 0.0571824055100569, 'senator': 0.0, 'husband': 0.0}\n",
      "Cosine distances for \"Spain\":\n",
      "\tSimilar: {'Madrid': 0.6707700464776317, 'Catalonia': 0.8567311328463756, 'Europe': 0.09002191295904333}\n",
      "\tRelated: {'Brexit': 0.0, 'kingdom': 0.0, 'EU': 0.09194621406122841}\n",
      "\tUnrelated: {'Trump': 0.0019796542475018055, 'Twitter': 0.0, 'Korea': 0.0}\n"
     ]
    }
   ],
   "source": [
    "for word in words_to_analyze:\n",
    "    word_vec = term_document_df.loc[word]\n",
    "    print(f'Cosine distances for \"{word}\":')\n",
    "    for group, words in [('Similar', similar_words), ('Related', related_words), ('Unrelated', unrelated_words)]:\n",
    "        distances = {target_word: cosine_similarity(word_vec, term_document_df.loc[target_word]) for target_word in words[word]}\n",
    "        print(f'\\t{group}: {distances}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13011, 30)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 30\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "reduced_tfidf_vectors = pca.fit_transform(term_document_df)\n",
    "reduced_tfidf_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_tfidf_vectors = pd.DataFrame.from_records(reduced_tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_tfidf_vectors.index = term_document_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distances for \"Monday\":\n",
      "\tSimilar: {'Tuesday': 0.7374015692207786, 'Wednesday': 0.7261263865982414, 'Thursday': 0.7507198845840246}\n",
      "\tRelated: {'weekend': 0.4181102156642323, 'day': 0.6678653211340354, 'week': 0.7115425382670025}\n",
      "\tUnrelated: {'funds': 0.2982809576613117, 'town': 0.6012269029751238, 'territory': 0.322054506222034}\n",
      "Cosine distances for \"north\":\n",
      "\tSimilar: {'south': 0.7375841909122571, 'west': 0.39240543947350165, 'east': 0.4744598954910153}\n",
      "\tRelated: {'world': 0.1522120057741268, 'side': 0.5345555764411302, 'direction': -0.06229777976009694}\n",
      "\tUnrelated: {'party': 0.033074286203962966, 'senator': -0.06636269470825455, 'husband': -0.11761922863692317}\n",
      "Cosine distances for \"Spain\":\n",
      "\tSimilar: {'Madrid': 0.8969491784283574, 'Catalonia': 0.9886405095367214, 'Europe': 0.2567045665840962}\n",
      "\tRelated: {'Brexit': 0.2712823734169814, 'kingdom': 0.019116605694875725, 'EU': 0.3203650429985352}\n",
      "\tUnrelated: {'Trump': -0.08437770172852076, 'Twitter': -0.10437311556304842, 'Korea': -0.028947963987412794}\n"
     ]
    }
   ],
   "source": [
    "for word in words_to_analyze:\n",
    "    word_vec = reduced_tfidf_vectors.loc[word]\n",
    "    print(f'Cosine distances for \"{word}\":')\n",
    "    for group, words in [('Similar', similar_words), ('Related', related_words), ('Unrelated', unrelated_words)]:\n",
    "        distances = {target_word: cosine_similarity(word_vec, reduced_tfidf_vectors.loc[target_word]) for target_word in words[word]}\n",
    "        print(f'\\t{group}: {distances}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_with_w2v(text, model):\n",
    "    tokenized_sentences = preprocess_text(text)\n",
    "    sentence_vectors = []\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        word_vectors = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word in model.wv.key_to_index:\n",
    "                word_vector = model.wv[word]\n",
    "                word_vectors.append(word_vector)\n",
    "                \n",
    "        if word_vectors:\n",
    "                sentence_vector = np.mean(word_vectors, axis=0)\n",
    "                sentence_vectors.append(sentence_vector)\n",
    "\n",
    "    if sentence_vectors:\n",
    "        document_vector = np.mean(sentence_vectors, axis=0)\n",
    "        return document_vector\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WASHINGTON Reuters U State Department certified Honduran government fighting corruption supporting human rights clearing way Honduras receive millions dollars U aid document seen Reuters showed document dated Nov 28 seen Reuters Monday showed Secretary State Rex Tillerson certified Honduras assistance two days controversial presidential election claimed ally Washington Honduras faced violent protests disputed results election still produced clear winner week vote ended decision issue certification prompted concern congressional Democrats Republican President Donald Trump administration could seen taking sides kind message send one congressional aide asked State Department officials immediate response questioned timing certification Honduras required fulfill dozen requirements order receive share million U Congress program assist Central American governments Among requirements combating corruption including investigating prosecuting current former government officials alleged corrupt protecting rights political opposition parties Honduras struggles violent drug gangs one world highest murder rates endemic poverty recent years many Hondurans including children attempted migrate United States hopes migration former President Barack Obama administration 2015 came plan included sending hundreds millions dollars additional aid Honduras Guatemala El Salvador Congress agreed provide money governments found taking steps fight crime corruption preliminary ballot count Honduras Monday pointed narrow victory President Juan Orlando Hernandez opposition challenger Salvador Nasralla although electoral tribunal declared winner Early last week Nasralla former game show host appeared set upset victory Hernandez counting process suddenly halted day began leaning favor Hernandez resuming Opposition leaders said wanted accused government stealing election Hernandez 49 implemented military -led crackdown gang violence taking office 2014 supported Trump chief staff John Kelly Nasralla 64 one Honduras best -known faces backed former President Manuel Zelaya leftist ousted coup 2009\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "text = ' '.join(train_texts[0])\n",
    "print(text)\n",
    "print(vectorize_with_w2v(text, model).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = [[token for token in ds['Token'].to_list() if is_valid_token(token, token_frequencies)] for ds in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectors = [vectorize_with_w2v(' '.join(text), model) for text in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vectors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../assets/annotated-corpus/test-embeddings.tsv', 'w') as file:\n",
    "    for doc_id, vector in zip(test_ids, test_vectors):\n",
    "        vector_str = '\\t'.join(map(str, vector))\n",
    "        file.write(f'{doc_id}\\t{vector_str}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

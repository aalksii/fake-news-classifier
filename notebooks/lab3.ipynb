{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory work #3 (text vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import string\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.errors import EmptyDataError\n",
    "import numpy as np\n",
    "from math import log1p\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(root_dir, n=None):\n",
    "    file_paths = []\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.tsv'):\n",
    "                file_paths.append(os.path.join(subdir, file))\n",
    "\n",
    "    data = []\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        if n is not None and i >= n:\n",
    "            break\n",
    "        try:\n",
    "            d = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "            d.columns = ['Token', 'Stem', 'Lemma']\n",
    "        except EmptyDataError as e:\n",
    "            print(i, file_path, e)\n",
    "        data.append(d.dropna())\n",
    "        \n",
    "    \n",
    "    ids = [os.path.splitext(os.path.basename(path))[0] for path in file_paths]\n",
    "    return ids, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9520 ../assets/annotated-corpus/train/fake/21080.tsv No columns to parse from file\n",
      "12465 ../assets/annotated-corpus/train/fake/31165.tsv No columns to parse from file\n",
      "2445 ../assets/annotated-corpus/test/true/22518.tsv No columns to parse from file\n"
     ]
    }
   ],
   "source": [
    "train_ids, train = read_files('../assets/annotated-corpus/train', \n",
    "                  #  1000\n",
    "                   )\n",
    "# val_ids, val = read_files('../assets/annotated-corpus/val', \n",
    "#                 #  100\n",
    "#                  )\n",
    "test_ids, test = read_files('../assets/annotated-corpus/test', \n",
    "                #   100\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Stem</th>\n",
       "      <th>Lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USA</td>\n",
       "      <td>usa</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Today</td>\n",
       "      <td>today</td>\n",
       "      <td>Today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>published</td>\n",
       "      <td>publish</td>\n",
       "      <td>published</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>article</td>\n",
       "      <td>articl</td>\n",
       "      <td>article</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Token     Stem      Lemma\n",
       "0        USA      usa        USA\n",
       "1      Today    today      Today\n",
       "2  published  publish  published\n",
       "3         an       an         an\n",
       "4    article   articl    article"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_token(token, token_frequencies, min_frequency=2):\n",
    "    if token in string.punctuation:\n",
    "        return False\n",
    "    if token.lower() in stop_words:\n",
    "        return False\n",
    "    if token_frequencies[token] < min_frequency:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_freqs(dfs):\n",
    "    token_frequencies = Counter()\n",
    "    term_document_matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for doc_id, df in enumerate(dfs):\n",
    "        tokens = df['Token'].tolist()\n",
    "        token_frequencies.update(tokens)\n",
    "\n",
    "        for token in tokens:\n",
    "            if is_valid_token(token, token_frequencies):\n",
    "                term_document_matrix[doc_id][token] += 1\n",
    "                \n",
    "    # filter all tokens that return is_valid_token False\n",
    "    token_frequencies = Counter(dict({(token, freq) for (token, freq) in token_frequencies.items() if is_valid_token(token, token_frequencies)}))\n",
    "\n",
    "    for doc_id, terms in term_document_matrix.items():\n",
    "        term_document_matrix[doc_id] = {token: freq for token, freq in terms.items() if is_valid_token(token, token_frequencies)}\n",
    "\n",
    "    return token_frequencies, term_document_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 98241),\n",
       " ('Trump', 96486),\n",
       " ('U', 39632),\n",
       " ('would', 39288),\n",
       " ('people', 27540),\n",
       " ('President', 24731),\n",
       " ('Reuters', 23085),\n",
       " ('one', 21932),\n",
       " ('also', 21062),\n",
       " ('Donald', 20538),\n",
       " ('Republican', 19150),\n",
       " ('government', 18758),\n",
       " ('House', 18317),\n",
       " ('Clinton', 18097),\n",
       " ('Obama', 18029),\n",
       " ('could', 16798),\n",
       " ('told', 16567),\n",
       " ('United', 16544),\n",
       " ('campaign', 15198),\n",
       " ('state', 15043)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_frequencies, term_document_matrix = get_freqs(train)\n",
    "token_frequencies.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'USA': 15,\n",
       " 'Today': 13,\n",
       " 'published': 2,\n",
       " 'article': 4,\n",
       " 'today': 2,\n",
       " 'every': 4,\n",
       " 'hotel': 2,\n",
       " 'publication': 2,\n",
       " 'guests': 2,\n",
       " 'read': 2,\n",
       " 'free': 3,\n",
       " 'one': 3,\n",
       " 'anti': 2,\n",
       " '-Trump': 2,\n",
       " 'find': 2,\n",
       " 'like': 2,\n",
       " 'morning': 2,\n",
       " 'whose': 2,\n",
       " 'president': 2,\n",
       " 'family': 9,\n",
       " 'almost': 4,\n",
       " 'news': 3,\n",
       " 'across': 2,\n",
       " 'American': 5,\n",
       " 'without': 2,\n",
       " 'situation': 2,\n",
       " 'President': 6,\n",
       " 'Trump': 7,\n",
       " 'unprecedented': 2,\n",
       " 'number': 3,\n",
       " 'White': 3,\n",
       " 'House': 3,\n",
       " 'protectees': 2,\n",
       " 'Secret': 13,\n",
       " 'Service': 13,\n",
       " 'Director': 5,\n",
       " 'Randolph': 4,\n",
       " 'Tex': 4,\n",
       " 'Alles': 7,\n",
       " 'statement': 3,\n",
       " 'said': 5,\n",
       " 'pay': 5,\n",
       " 'hundreds': 2,\n",
       " 'agents': 6,\n",
       " 'needs': 3,\n",
       " 'mission': 3,\n",
       " 'large': 2,\n",
       " 'due': 2,\n",
       " 'funding': 2,\n",
       " 'meet': 2,\n",
       " 'current': 2,\n",
       " 'requirements': 4,\n",
       " 'year': 5,\n",
       " 'employees': 4,\n",
       " 'overtime': 4,\n",
       " 'statutory': 3,\n",
       " 'caps': 4,\n",
       " 'TODAY': 2,\n",
       " '1': 2,\n",
       " '000': 3,\n",
       " 'already': 5,\n",
       " 'salary': 2,\n",
       " 'entire': 3,\n",
       " '2016': 2,\n",
       " 'Barack': 3,\n",
       " 'Obama': 4,\n",
       " 'work': 4,\n",
       " 'hours': 3,\n",
       " 'calendar': 2,\n",
       " '2017': 2,\n",
       " 'agency': 4,\n",
       " 'first': 4,\n",
       " 'months': 2,\n",
       " 'administration': 2,\n",
       " 'traveled': 2,\n",
       " 'weekend': 2,\n",
       " 'vacations': 2,\n",
       " 'country': 2,\n",
       " 'overseas': 2,\n",
       " 'ongoing': 2,\n",
       " 'serious': 2,\n",
       " 'Administration': 2,\n",
       " 'several': 2,\n",
       " 'compensated': 2,\n",
       " 'law': 2,\n",
       " '18': 2,\n",
       " 'protection': 3,\n",
       " 'members': 3,\n",
       " 'people': 2,\n",
       " 'believe': 2,\n",
       " 'forgotten': 2,\n",
       " 'former': 2,\n",
       " 'took': 2,\n",
       " 'courtesy': 2,\n",
       " 'luxury': 2,\n",
       " 'African': 2,\n",
       " 'Safari': 2,\n",
       " 'taxpayer': 2,\n",
       " 'term': 2,\n",
       " 'time': 2,\n",
       " 'travel': 2,\n",
       " 'even': 2,\n",
       " 'issue': 2,\n",
       " 'caught': 2,\n",
       " 'fabricating': 2,\n",
       " 'story': 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_document_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../assets/data/')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "with open(data_dir / 'token_frequencies.tsv', 'w', encoding='utf-8') as file:\n",
    "    for token, freq in token_frequencies.items():\n",
    "        if is_valid_token(token, token_frequencies):\n",
    "            file.write(f'{token}\\t{freq}\\n')\n",
    "\n",
    "with open(data_dir / 'term_document_matrix.tsv', 'w', encoding='utf-8') as file:\n",
    "    for doc_id, terms in term_document_matrix.items():\n",
    "        for token, freq in terms.items():\n",
    "            file.write(f'{doc_id}\\t{token}\\t{freq}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../assets/data/')\n",
    "\n",
    "token_frequencies = {}\n",
    "with open(data_dir / 'token_frequencies.tsv', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        token, freq = line.strip().split('\\t')\n",
    "        token_frequencies[token] = int(freq)\n",
    "\n",
    "term_document_matrix = {}\n",
    "with open(data_dir / 'term_document_matrix.tsv', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        doc_id, token, freq = line.strip().split('\\t')\n",
    "        doc_id = int(doc_id)\n",
    "        freq = int(freq)\n",
    "        if doc_id not in term_document_matrix:\n",
    "            term_document_matrix[doc_id] = {}\n",
    "        term_document_matrix[doc_id][token] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'USA': 15,\n",
       " 'Today': 13,\n",
       " 'published': 2,\n",
       " 'article': 4,\n",
       " 'today': 2,\n",
       " 'every': 4,\n",
       " 'hotel': 2,\n",
       " 'publication': 2,\n",
       " 'guests': 2,\n",
       " 'read': 2,\n",
       " 'free': 3,\n",
       " 'one': 3,\n",
       " 'anti': 2,\n",
       " '-Trump': 2,\n",
       " 'find': 2,\n",
       " 'like': 2,\n",
       " 'morning': 2,\n",
       " 'whose': 2,\n",
       " 'president': 2,\n",
       " 'family': 9,\n",
       " 'almost': 4,\n",
       " 'news': 3,\n",
       " 'across': 2,\n",
       " 'American': 5,\n",
       " 'without': 2,\n",
       " 'situation': 2,\n",
       " 'President': 6,\n",
       " 'Trump': 7,\n",
       " 'unprecedented': 2,\n",
       " 'number': 3,\n",
       " 'White': 3,\n",
       " 'House': 3,\n",
       " 'protectees': 2,\n",
       " 'Secret': 13,\n",
       " 'Service': 13,\n",
       " 'Director': 5,\n",
       " 'Randolph': 4,\n",
       " 'Tex': 4,\n",
       " 'Alles': 7,\n",
       " 'statement': 3,\n",
       " 'said': 5,\n",
       " 'pay': 5,\n",
       " 'hundreds': 2,\n",
       " 'agents': 6,\n",
       " 'needs': 3,\n",
       " 'mission': 3,\n",
       " 'large': 2,\n",
       " 'due': 2,\n",
       " 'funding': 2,\n",
       " 'meet': 2,\n",
       " 'current': 2,\n",
       " 'requirements': 4,\n",
       " 'year': 5,\n",
       " 'employees': 4,\n",
       " 'overtime': 4,\n",
       " 'statutory': 3,\n",
       " 'caps': 4,\n",
       " 'TODAY': 2,\n",
       " '1': 2,\n",
       " '000': 3,\n",
       " 'already': 5,\n",
       " 'salary': 2,\n",
       " 'entire': 3,\n",
       " '2016': 2,\n",
       " 'Barack': 3,\n",
       " 'Obama': 4,\n",
       " 'work': 4,\n",
       " 'hours': 3,\n",
       " 'calendar': 2,\n",
       " '2017': 2,\n",
       " 'agency': 4,\n",
       " 'first': 4,\n",
       " 'months': 2,\n",
       " 'administration': 2,\n",
       " 'traveled': 2,\n",
       " 'weekend': 2,\n",
       " 'vacations': 2,\n",
       " 'country': 2,\n",
       " 'overseas': 2,\n",
       " 'ongoing': 2,\n",
       " 'serious': 2,\n",
       " 'Administration': 2,\n",
       " 'several': 2,\n",
       " 'compensated': 2,\n",
       " 'law': 2,\n",
       " '18': 2,\n",
       " 'protection': 3,\n",
       " 'members': 3,\n",
       " 'people': 2,\n",
       " 'believe': 2,\n",
       " 'forgotten': 2,\n",
       " 'former': 2,\n",
       " 'took': 2,\n",
       " 'courtesy': 2,\n",
       " 'luxury': 2,\n",
       " 'African': 2,\n",
       " 'Safari': 2,\n",
       " 'taxpayer': 2,\n",
       " 'term': 2,\n",
       " 'time': 2,\n",
       " 'travel': 2,\n",
       " 'even': 2,\n",
       " 'issue': 2,\n",
       " 'caught': 2,\n",
       " 'fabricating': 2,\n",
       " 'story': 3}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_document_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_document_vector(token, term_document_matrix):\n",
    "    vector = []\n",
    "    for k, v in term_document_matrix.items():\n",
    "        freq = v.get(token, 0)\n",
    "        vector.append(freq)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_term_document_vector('Reuters', term_document_matrix)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_term_document_vector('cat', term_document_matrix)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    def split_into_sentences(text):\n",
    "        # so the website will not split into two separate sentences by comma:\n",
    "        sentence_endings = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!)(?=\\s|[#])')\n",
    "        sentences = sentence_endings.split(text)\n",
    "        sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "        return sentences\n",
    "    \n",
    "    def split_into_words(sentences):\n",
    "        # regular expression to match complex URLs, simple URLs, hashtags, Twitter handles, and words\n",
    "        word_pattern = re.compile(r'pic.twitter.com/\\S+|https?://\\S+|www\\.\\S+|\\#\\S+|\\@\\w+|\\b\\w+\\'?\\w*|-?\\w+\\'?\\w*')\n",
    "        tokenized_sentences = []\n",
    "        for sentence in sentences:\n",
    "            words = word_pattern.findall(sentence)\n",
    "            tokenized_sentences.append(words)\n",
    "        return tokenized_sentences\n",
    "\n",
    "    sentences = split_into_sentences(text)\n",
    "    tokenized = split_into_words(sentences)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(sentence_tokens, token):\n",
    "    return sentence_tokens.count(token) / len(sentence_tokens)\n",
    "\n",
    "\n",
    "def compute_idf(token, term_document_matrix, total_documents):\n",
    "    doc_count = sum(1 for doc in term_document_matrix if token in term_document_matrix[doc])\n",
    "    return log1p(total_documents / (1 + doc_count))\n",
    "\n",
    "\n",
    "def process_text_and_create_matrices(text, token_frequencies, term_document_matrix):\n",
    "    tokenized_sentences = preprocess_text(text)\n",
    "    total_documents = len(term_document_matrix)\n",
    "    vocabulary = sorted(token_frequencies.keys())\n",
    "\n",
    "    max_sentence_length = max(len(sentence) for sentence in tokenized_sentences)\n",
    "\n",
    "    frequency_matrix = []\n",
    "    tfidf_matrix = []\n",
    "\n",
    "    for sentence in tokenized_sentences:\n",
    "        sentence_freq_vector = [0] * max_sentence_length\n",
    "        sentence_tfidf_vector = [0] * max_sentence_length\n",
    "\n",
    "        for i, token in enumerate(sentence):\n",
    "            if token in vocabulary:\n",
    "                tf = compute_tf(sentence, token)\n",
    "                idf = compute_idf(token, term_document_matrix, total_documents)\n",
    "\n",
    "                sentence_freq_vector[i] = tf\n",
    "                sentence_tfidf_vector[i] = tf * idf\n",
    "\n",
    "        frequency_matrix.append(sentence_freq_vector)\n",
    "        tfidf_matrix.append(sentence_tfidf_vector)\n",
    "\n",
    "    frequency_matrix = np.array(frequency_matrix)\n",
    "    tfidf_matrix = np.array(tfidf_matrix)\n",
    "\n",
    "    document_vector_freq = np.mean(frequency_matrix, axis=0)\n",
    "    document_vector_tfidf = np.mean(tfidf_matrix, axis=0)\n",
    "\n",
    "    return document_vector_freq, document_vector_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boos and chants of  Lock her up!  were heard in the crowd assembled at the West Front of the U.S. Capitol Friday morning when defeated Democratic Party presidential nominee Hillary Clinton was introduced at the inaugural ceremony for President-elect Donald Trump.#InaugurationDay Lock her up pic.twitter.com/APVtyyYote  Bill Simms (@Mittens1245) January 20, 2017The crowd on the mall booed when the jumbotron showed a close-up shot of Hillary Clinton at #Inauguration https://t.co/1dvY5lxdKo  gpbnews (@gpbnews) January 20, 2017Some in crowd chanting LOCK HER UP as Hillary Clinton arrives  Jamie Dupree (@jamiedupree) January 20, 2017Via: Gateway Pundit \n"
     ]
    }
   ],
   "source": [
    "text = 'Boos and chants of  Lock her up!  were heard in the crowd assembled at the West Front of the U.S. Capitol Friday morning when defeated Democratic Party presidential nominee Hillary Clinton was introduced at the inaugural ceremony for President-elect Donald Trump.#InaugurationDay Lock her up pic.twitter.com/APVtyyYote  Bill Simms (@Mittens1245) January 20, 2017The crowd on the mall booed when the jumbotron showed a close-up shot of Hillary Clinton at #Inauguration https://t.co/1dvY5lxdKo  gpbnews (@gpbnews) January 20, 2017Some in crowd chanting LOCK HER UP as Hillary Clinton arrives  Jamie Dupree (@jamiedupree) January 20, 2017Via: Gateway Pundit '\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((53,), (53,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vector_freq, document_vector_tfidf = process_text_and_create_matrices(text, token_frequencies, term_document_matrix)\n",
    "document_vector_freq.shape, document_vector_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05390836, 0.01554857, 0.04761905, 0.        , 0.05687831,\n",
       "       0.01554857, 0.        , 0.        , 0.02812718, 0.02812718,\n",
       "       0.00628931, 0.01257862, 0.00925926, 0.        , 0.01554857,\n",
       "       0.01554857, 0.00925926, 0.        , 0.00925926, 0.01554857,\n",
       "       0.00925926, 0.01554857, 0.01554857, 0.01554857, 0.00925926,\n",
       "       0.01257862, 0.02183788, 0.        , 0.00628931, 0.00925926,\n",
       "       0.00925926, 0.        , 0.02812718, 0.02812718, 0.01554857,\n",
       "       0.00925926, 0.01257862, 0.00628931, 0.00628931, 0.        ,\n",
       "       0.        , 0.        , 0.01257862, 0.01257862, 0.00628931,\n",
       "       0.00628931, 0.00628931, 0.        , 0.01886792, 0.01886792,\n",
       "       0.00628931, 0.00628931, 0.00628931])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vector_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.51415463, 0.07121161, 0.27656899, 0.        , 0.34779873,\n",
       "       0.07254365, 0.        , 0.        , 0.08302616, 0.09588796,\n",
       "       0.0290603 , 0.04229549, 0.01115623, 0.        , 0.07728029,\n",
       "       0.05543456, 0.02773684, 0.        , 0.0420934 , 0.03519483,\n",
       "       0.02184315, 0.03266809, 0.04616814, 0.03875599, 0.01840298,\n",
       "       0.02506917, 0.06191121, 0.        , 0.04842098, 0.05248307,\n",
       "       0.04153007, 0.        , 0.06205771, 0.07746551, 0.0615242 ,\n",
       "       0.01006755, 0.04229549, 0.03267674, 0.05374827, 0.        ,\n",
       "       0.        , 0.        , 0.02506917, 0.02500028, 0.04017597,\n",
       "       0.03639457, 0.05810708, 0.        , 0.05138533, 0.04699861,\n",
       "       0.05810708, 0.03293664, 0.03336833])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vector_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [[token for token in ds['Token'].to_list() if token in token_frequencies.keys() and is_valid_token(token, token_frequencies)] for ds in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['USA',\n",
       " 'Today',\n",
       " 'published',\n",
       " 'article',\n",
       " 'today',\n",
       " 'egregiously',\n",
       " 'misleading',\n",
       " 'every',\n",
       " 'hotel',\n",
       " 'leaves']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences=train_texts, vector_size=30, window=5, min_count=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('../models/').mkdir(parents=True, exist_ok=True)\n",
    "model_path = '../models/word2vec.model'\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 6980\n",
      "Close: 7964 7500 7296\n",
      "Same area 1193 5844 9020\n",
      "Other semantic 1550 1575 994\n"
     ]
    }
   ],
   "source": [
    "print('Word:', token_frequencies['Monday'])\n",
    "print('Close:', token_frequencies['Tuesday'], token_frequencies['Wednesday'], token_frequencies['Thursday'])\n",
    "print('Same area', token_frequencies['weekend'], token_frequencies['day'], token_frequencies['week'])\n",
    "print('Other semantic', token_frequencies['funds'], token_frequencies['town'], token_frequencies['territory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 615\n",
      "Close: 569 377 419\n",
      "Same area 6777 2008 559\n",
      "Other semantic 9548 1384 1331\n"
     ]
    }
   ],
   "source": [
    "print('Word:', token_frequencies['north'])\n",
    "print('Close:', token_frequencies['south'], token_frequencies['west'], token_frequencies['east'])\n",
    "print('Same area', token_frequencies['world'], token_frequencies['side'], token_frequencies['direction'])\n",
    "print('Other semantic', token_frequencies['party'], token_frequencies['senator'], token_frequencies['husband'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 955\n",
      "Close: 437 771 2270\n",
      "Same area 1492 291 4043\n",
      "Other semantic 96486 5347 6240\n"
     ]
    }
   ],
   "source": [
    "print('Word:', token_frequencies['Spain'])\n",
    "print('Close:', token_frequencies['Madrid'], token_frequencies['Catalonia'], token_frequencies['Europe'])\n",
    "print('Same area', token_frequencies['Brexit'], token_frequencies['kingdom'], token_frequencies['EU'])\n",
    "print('Other semantic', token_frequencies['Trump'], token_frequencies['Twitter'], token_frequencies['Korea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distances for \"Monday\":\n",
      "\tSimilar: {'Tuesday': 0.9737339, 'Wednesday': 0.9858618, 'Thursday': 0.982888}\n",
      "\tRelated: {'weekend': 0.56576025, 'day': 0.46691594, 'week': 0.5616831}\n",
      "\tUnrelated: {'funds': 0.049275674, 'town': 0.33820704, 'territory': -0.00015503431}\n",
      "Cosine distances for \"north\":\n",
      "\tSimilar: {'south': 0.94738543, 'west': 0.95018405, 'east': 0.9475872}\n",
      "\tRelated: {'world': 0.2151862, 'side': 0.24820313, 'direction': -0.025277833}\n",
      "\tUnrelated: {'party': 0.033860423, 'senator': -0.18111715, 'husband': -0.1852833}\n",
      "Cosine distances for \"Spain\":\n",
      "\tSimilar: {'Madrid': 0.8731296, 'Catalonia': 0.9340388, 'Europe': 0.5712906}\n",
      "\tRelated: {'Brexit': 0.5921776, 'kingdom': 0.27127314, 'EU': 0.59468395}\n",
      "\tUnrelated: {'Trump': -0.14517818, 'Twitter': -0.21532726, 'Korea': 0.03403758}\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec_a, vec_b):\n",
    "    dot_product = np.dot(vec_a, vec_b)\n",
    "    norm_a = np.linalg.norm(vec_a)\n",
    "    norm_b = np.linalg.norm(vec_b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "words_to_analyze = ['Monday', 'north', 'Spain']\n",
    "similar_words = {\n",
    "    'Monday': ['Tuesday', 'Wednesday', 'Thursday'], \n",
    "    'north': ['south', 'west', 'east'],\n",
    "    'Spain': ['Madrid', 'Catalonia', 'Europe']\n",
    "}\n",
    "\n",
    "related_words = {\n",
    "    'Monday': ['weekend', 'day', 'week'], \n",
    "    'north': ['world', 'side', 'direction'],\n",
    "    'Spain': ['Brexit', 'kingdom', 'EU']\n",
    "}\n",
    "\n",
    "unrelated_words = {\n",
    "    'Monday': ['funds', 'town', 'territory'], \n",
    "    'north': ['party', 'senator', 'husband'],\n",
    "    'Spain': ['Trump', 'Twitter', 'Korea']\n",
    "}\n",
    "\n",
    "for word in words_to_analyze:\n",
    "    word_vec = model.wv[word]\n",
    "    print(f'Cosine distances for \"{word}\":')\n",
    "    for group, words in [('Similar', similar_words), ('Related', related_words), ('Unrelated', unrelated_words)]:\n",
    "        distances = {target_word: cosine_similarity(word_vec, model.wv[target_word]) for target_word in words[word]}\n",
    "        print(f'\\t{group}: {distances}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distances for \"Monday\":\n",
      "\tSimilar: {'Tuesday': 0.21058447533038976, 'Wednesday': 0.09644222814763143, 'Thursday': 0.0839318871089539}\n",
      "\tRelated: {'weekend': 0.13976112455496112, 'day': 0.1461953071514957, 'week': 0.23562990612492185}\n",
      "\tUnrelated: {'funds': 0.05497001853953496, 'town': 0.05789731268175128, 'territory': 0.07635427079184337}\n",
      "Cosine distances for \"north\":\n",
      "\tSimilar: {'south': 0.22760009183170773, 'west': 0.17512123312849956, 'east': 0.10182192622997722}\n",
      "\tRelated: {'world': 0.03645647388968974, 'side': 0.07851380774439148, 'direction': 0.03168753817050964}\n",
      "\tUnrelated: {'party': 0.030857041733275928, 'senator': 0.005178081383343333, 'husband': 0.014101017404050889}\n",
      "Cosine distances for \"Spain\":\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSimilar: {'Madrid': 0.6871109854186562, 'Catalonia': 0.7871924013529985, 'Europe': 0.05470590470976215}\n",
      "\tRelated: {'Brexit': 0.004884680873595118, 'kingdom': 0.0031832043942822408, 'EU': 0.07276382992608514}\n",
      "\tUnrelated: {'Trump': 0.004025144580344135, 'Twitter': 0.01138766491923137, 'Korea': 0.004247065514542696}\n"
     ]
    }
   ],
   "source": [
    "for word in words_to_analyze:\n",
    "    word_vec = get_term_document_vector(word, term_document_matrix)\n",
    "    print(f'Cosine distances for \"{word}\":')\n",
    "    for group, words in [('Similar', similar_words), ('Related', related_words), ('Unrelated', unrelated_words)]:\n",
    "        distances = {target_word: cosine_similarity(word_vec, get_term_document_vector(target_word, term_document_matrix)) for target_word in words[word]}\n",
    "        print(f'\\t{group}: {distances}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_document_df = np.zeros((len(token_frequencies), len(term_document_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78759, 30870)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_document_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n"
     ]
    }
   ],
   "source": [
    "for i, term in enumerate(token_frequencies.keys()):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    term_document_df[i, :] = np.array(get_term_document_vector(term, term_document_matrix), dtype=np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term_document_df_ = term_document_df[:5000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78759, 30)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "n_components = 30\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "reduced_tfidf_vectors = pca.fit_transform(term_document_df)\n",
    "reduced_tfidf_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../assets/reduced_tfidf_vectors.npy', reduced_tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_tfidf_vectors = pd.DataFrame.from_records(reduced_tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_tfidf_vectors.index = list(token_frequencies.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bleach</th>\n",
       "      <td>-0.530313</td>\n",
       "      <td>0.193183</td>\n",
       "      <td>0.133454</td>\n",
       "      <td>0.080183</td>\n",
       "      <td>0.065210</td>\n",
       "      <td>-0.004830</td>\n",
       "      <td>0.048303</td>\n",
       "      <td>0.029062</td>\n",
       "      <td>-0.034110</td>\n",
       "      <td>0.022189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012084</td>\n",
       "      <td>0.005808</td>\n",
       "      <td>-0.000485</td>\n",
       "      <td>-0.010206</td>\n",
       "      <td>0.007498</td>\n",
       "      <td>-0.008831</td>\n",
       "      <td>0.018373</td>\n",
       "      <td>-0.012391</td>\n",
       "      <td>-0.011868</td>\n",
       "      <td>-0.016482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>councilor</th>\n",
       "      <td>-0.524775</td>\n",
       "      <td>0.043473</td>\n",
       "      <td>0.194606</td>\n",
       "      <td>0.103258</td>\n",
       "      <td>0.041672</td>\n",
       "      <td>-0.026342</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>-0.006103</td>\n",
       "      <td>-0.045053</td>\n",
       "      <td>-0.035502</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003385</td>\n",
       "      <td>-0.046530</td>\n",
       "      <td>-0.053155</td>\n",
       "      <td>0.044384</td>\n",
       "      <td>-0.013902</td>\n",
       "      <td>0.012952</td>\n",
       "      <td>0.087045</td>\n",
       "      <td>0.063328</td>\n",
       "      <td>-0.031316</td>\n",
       "      <td>0.022185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beachgoers</th>\n",
       "      <td>-0.596811</td>\n",
       "      <td>0.135316</td>\n",
       "      <td>0.182437</td>\n",
       "      <td>0.107843</td>\n",
       "      <td>0.032547</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>0.013508</td>\n",
       "      <td>0.022382</td>\n",
       "      <td>-0.028621</td>\n",
       "      <td>0.017486</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013817</td>\n",
       "      <td>-0.000462</td>\n",
       "      <td>-0.011339</td>\n",
       "      <td>-0.003229</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>-0.003257</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.006532</td>\n",
       "      <td>-0.015685</td>\n",
       "      <td>0.003926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>complexities</th>\n",
       "      <td>-0.513003</td>\n",
       "      <td>0.047696</td>\n",
       "      <td>0.218139</td>\n",
       "      <td>0.108336</td>\n",
       "      <td>0.030107</td>\n",
       "      <td>0.007141</td>\n",
       "      <td>-0.049194</td>\n",
       "      <td>-0.017400</td>\n",
       "      <td>-0.051327</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031288</td>\n",
       "      <td>0.028371</td>\n",
       "      <td>0.025444</td>\n",
       "      <td>0.073110</td>\n",
       "      <td>-0.023945</td>\n",
       "      <td>-0.053308</td>\n",
       "      <td>0.017267</td>\n",
       "      <td>-0.001586</td>\n",
       "      <td>0.023437</td>\n",
       "      <td>-0.036362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immaculate</th>\n",
       "      <td>-0.597218</td>\n",
       "      <td>0.136648</td>\n",
       "      <td>0.183605</td>\n",
       "      <td>0.108875</td>\n",
       "      <td>0.031585</td>\n",
       "      <td>0.006128</td>\n",
       "      <td>0.009378</td>\n",
       "      <td>0.029540</td>\n",
       "      <td>-0.027701</td>\n",
       "      <td>0.018066</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016245</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>-0.019077</td>\n",
       "      <td>-0.009499</td>\n",
       "      <td>-0.002077</td>\n",
       "      <td>-0.016662</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.014233</td>\n",
       "      <td>-0.020286</td>\n",
       "      <td>-0.001692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0         1         2         3         4         5   \\\n",
       "bleach       -0.530313  0.193183  0.133454  0.080183  0.065210 -0.004830   \n",
       "councilor    -0.524775  0.043473  0.194606  0.103258  0.041672 -0.026342   \n",
       "beachgoers   -0.596811  0.135316  0.182437  0.107843  0.032547  0.001504   \n",
       "complexities -0.513003  0.047696  0.218139  0.108336  0.030107  0.007141   \n",
       "immaculate   -0.597218  0.136648  0.183605  0.108875  0.031585  0.006128   \n",
       "\n",
       "                    6         7         8         9   ...        20        21  \\\n",
       "bleach        0.048303  0.029062 -0.034110  0.022189  ...  0.012084  0.005808   \n",
       "councilor     0.028400 -0.006103 -0.045053 -0.035502  ... -0.003385 -0.046530   \n",
       "beachgoers    0.013508  0.022382 -0.028621  0.017486  ... -0.013817 -0.000462   \n",
       "complexities -0.049194 -0.017400 -0.051327  0.002307  ... -0.031288  0.028371   \n",
       "immaculate    0.009378  0.029540 -0.027701  0.018066  ... -0.016245  0.000813   \n",
       "\n",
       "                    22        23        24        25        26        27  \\\n",
       "bleach       -0.000485 -0.010206  0.007498 -0.008831  0.018373 -0.012391   \n",
       "councilor    -0.053155  0.044384 -0.013902  0.012952  0.087045  0.063328   \n",
       "beachgoers   -0.011339 -0.003229  0.003759 -0.003257  0.000793  0.006532   \n",
       "complexities  0.025444  0.073110 -0.023945 -0.053308  0.017267 -0.001586   \n",
       "immaculate   -0.019077 -0.009499 -0.002077 -0.016662  0.001050  0.014233   \n",
       "\n",
       "                    28        29  \n",
       "bleach       -0.011868 -0.016482  \n",
       "councilor    -0.031316  0.022185  \n",
       "beachgoers   -0.015685  0.003926  \n",
       "complexities  0.023437 -0.036362  \n",
       "immaculate   -0.020286 -0.001692  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_tfidf_vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distances for \"Monday\":\n",
      "\tSimilar: {'Tuesday': 0.9463421027556389, 'Wednesday': 0.95330938028016, 'Thursday': 0.9498681940518182}\n",
      "\tRelated: {'weekend': 0.7917631504620743, 'day': 0.6689732843296009, 'week': 0.922784135446442}\n",
      "\tUnrelated: {'funds': 0.47365928786747735, 'town': 0.5540664941481236, 'territory': 0.39525033605907406}\n",
      "Cosine distances for \"north\":\n",
      "\tSimilar: {'south': 0.9521470690765715, 'west': 0.84738123213278, 'east': 0.8190188816827702}\n",
      "\tRelated: {'world': 0.2527760210443455, 'side': 0.5479573714605677, 'direction': 0.2820184757323661}\n",
      "\tUnrelated: {'party': 0.17675241508013234, 'senator': 0.0188781305024143, 'husband': 0.1370231708485331}\n",
      "Cosine distances for \"Spain\":\n",
      "\tSimilar: {'Madrid': 0.9736261074047513, 'Catalonia': 0.9760077401932337, 'Europe': 0.4468716645848769}\n",
      "\tRelated: {'Brexit': 0.2684848346240149, 'kingdom': 0.1617980761620471, 'EU': 0.2627454083935922}\n",
      "\tUnrelated: {'Trump': -0.0017539808014249952, 'Twitter': 0.05002026298625143, 'Korea': 0.020863465795408615}\n"
     ]
    }
   ],
   "source": [
    "for word in words_to_analyze:\n",
    "    try:\n",
    "        word_vec = reduced_tfidf_vectors.loc[word]\n",
    "        print(f'Cosine distances for \"{word}\":')\n",
    "        for group, words in [('Similar', similar_words), ('Related', related_words), ('Unrelated', unrelated_words)]:\n",
    "            distances = {target_word: cosine_similarity(word_vec, reduced_tfidf_vectors.loc[target_word]) for target_word in words[word]}\n",
    "            print(f'\\t{group}: {distances}')\n",
    "    except:\n",
    "        print('no words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_with_w2v(text, model):\n",
    "    tokenized_sentences = preprocess_text(text)\n",
    "    sentence_vectors = []\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        word_vectors = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word in model.wv.key_to_index:\n",
    "                word_vector = model.wv[word]\n",
    "                word_vectors.append(word_vector)\n",
    "                \n",
    "        if word_vectors:\n",
    "                sentence_vector = np.mean(word_vectors, axis=0)\n",
    "                sentence_vectors.append(sentence_vector)\n",
    "\n",
    "    if sentence_vectors:\n",
    "        document_vector = np.mean(sentence_vectors, axis=0)\n",
    "        return document_vector\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USA Today published article today egregiously misleading every hotel leaves joke publication outside doors guests read ashamed peddling trash Even free know say free free reason USA Today one virulently anti -Trump publications find anywhere Much like morning shows whose morning talk hosts greet viewers cheerful dispositions big white smiles sticking president family USA Today found almost every hotel America complimentary source news guests USA Today comes across everyday American friendly publication gives Americans snapshot latest national news without bias Unfortunately nothing could truth USA Today stories anti -Trump almost come warning label anyone looking honest journalism unbiased news article appeared today USA Today talked dire financial situation President Trump family caused unprecedented number White House protectees Shortly article published Secret Service Director Randolph Tex Alles released statement refuting USA Today claimed said quotes red USA Today article Secret Service Director statement USA Today Secret Service longer pay hundreds agents needs carry expanded protective mission large part due sheer size President Trump family efforts necessary secure multiple residences East Coast Secret Service Director Randolph Tex Alles said Secret Service running money Secret Service funding needs meet current mission requirements remainder fiscal year compensate employees overtime within statutory pay caps Secret Service Director Randolph Tex Alles interview USA TODAY said 1 000 agents already hit federally mandated caps salary overtime allowances meant last entire year OOPS looks like thing happened 2016 Barack Obama President Secret Service estimates roughly 1 100 employees work overtime hours excess statutory pay caps calendar year 2017 agency experienced similar situation calendar year 2016 resulted legislation allowed Secret Service employees exceed statutory caps pay agency faced crushing workload since height contentious election season relented first seven months administration Agents must protect Trump traveled almost every weekend properties Florida New Jersey Virginia adult children whose business trips vacations taken across country overseas remedy ongoing serious problem agency worked closely Department Homeland Security Administration Congress past several months find legislative solution work ensure employees compensated hours work Secret Service continues rigorous hiring special agents Uniformed Division officers critical support staff meet future mission requirements president large family responsibility required law Alles said change flexibility law Title 18 US Code details protection requirements President Vice President immediate family members Alles said service grappling unprecedented number White House protectees Trump 42 people protection number includes 18 members family 31 Obama administration USA Today really believe average American stupid already forgotten several separate vacations former first lady took husband golfed almost every weekend office notorious trip Spain brought entire friends family list courtesy American taxpayers people see surrounding former First Lady Michele Obama luxury African Safari Michelle family members Yes American taxpayer paid luxury African Safari brother cousins course mother lived White House Barack entire 8 -year term traveled overseas destinations courtesy guessed American taxpayer Surely USA Today believe already forgotten Barack Obama obsession golf one time took lovely wife opposite sides country satisfy individual 5 -star travel needs constant travel also driving recent exodus Secret Service ranks yet without congressional intervention provide additional funding Alles even able pay agents work already done compensation crunch serious director begun discussions key lawmakers raise combined salary overtime cap agents 160 000 per year 187 000 least duration Trump first term even proposal approved 130 veteran agents would fully compensated hundreds hours already amassed according agency finally Secret Service Director Randolph Tex Alles allegedly gave USA Today exclusive scoop Trump family draining Secret Service funds dry concluded statement issue one attributed current Administration protection requirements alone rather ongoing issue nearly decade due overall increase operational tempo first time USA Today caught fabricating story make President Trump look bad readers April 2017 USA TODAY caught fabricating story innocent Dreamer deported Click read story\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "text = ' '.join(train_texts[0])\n",
    "print(text)\n",
    "print(vectorize_with_w2v(text, model).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = [[token for token in ds['Token'].to_list() if token in token_frequencies.keys() and is_valid_token(token, token_frequencies)] for ds in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One',\n",
       " 'major',\n",
       " 'things',\n",
       " 'Donald',\n",
       " 'Trump',\n",
       " 'speech',\n",
       " 'NATO',\n",
       " 'summit',\n",
       " 'shocked',\n",
       " 'everyone']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectors = [vectorize_with_w2v(' '.join(text), model) for text in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vectors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../assets/annotated-corpus/test-embeddings.tsv', 'w') as file:\n",
    "    for doc_id, vector in zip(test_ids, test_vectors):\n",
    "        vector_str = '\\t'.join(map(str, vector))\n",
    "        file.write(f'{doc_id}\\t{vector_str}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
